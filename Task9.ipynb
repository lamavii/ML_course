{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ваша задача — реализовать алгоритм случайного леса для задачи классификации и применить его к набору данных.\n",
    "\n",
    "Шаги выполнения задания:\n",
    "\n",
    "1. Импортируйте необходимые библиотеки: numpy, pandas, sklearn.\n",
    "\n",
    "2. Загрузите набор данных для задачи классификации. Вы можете использовать любой доступный набор данных или выбрать один из популярных, таких как Iris, Wine или MNIST.\n",
    "\n",
    "3. Проведите предварительную обработку данных, включая масштабирование и разделение на обучающую и тестовую выборки.\n",
    "\n",
    "4. Реализуйте алгоритм случайного леса. Включите в него функции для построения деревьев решений, выбора случайных признаков и голосования для принятия решений.\n",
    "\n",
    "5. Обучите вашу модель случайного леса на обучающей выборке.\n",
    "\n",
    "6. Оцените производительность модели на тестовой выборке, используя метрики классификации, такие как точность, полнота и F1-мера.\n",
    "\n",
    "7. Проведите сравнение результатов вашей модели со стандартной реализацией случайного леса из библиотеки scikit-learn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### РЕШЕНИЕ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем необходимые библиотеки для работы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Загружаем набор данных Iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предварительная обработка данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Масштабирование данных\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализация алгоритма случайного леса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь создадим класс для случайного леса. Реализация будет включать создание деревьев и голосование."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение модели случайного леса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y)\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        num_samples, num_features = X.shape\n",
    "        unique_classes = np.unique(y)\n",
    "        \n",
    "        # Если все классы одинаковые, возвращаем их\n",
    "        if len(unique_classes) == 1:\n",
    "            return unique_classes[0]\n",
    "        \n",
    "        # Если достигли максимальной глубины, возвращаем моду\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            return self._majority_class(y)\n",
    "\n",
    "        # Поиск наилучшего разбиения\n",
    "        best_gain = -1\n",
    "        best_split = None\n",
    "        best_left_indices = None\n",
    "        best_right_indices = None\n",
    "\n",
    "        for feature_index in range(num_features):  # Перебор всех признаков\n",
    "            thresholds = np.unique(X[:, feature_index])\n",
    "            for threshold in thresholds:  # Перебор всех возможных значений разбиения\n",
    "                left_indices = np.where(X[:, feature_index] <= threshold)[0]\n",
    "                right_indices = np.where(X[:, feature_index] > threshold)[0]\n",
    "\n",
    "                if len(left_indices) > 0 and len(right_indices) > 0:\n",
    "                    gain = self._information_gain(y, left_indices, right_indices)\n",
    "\n",
    "                    if gain > best_gain:\n",
    "                        best_gain = gain\n",
    "                        best_split = (feature_index, threshold)\n",
    "                        best_left_indices = left_indices\n",
    "                        best_right_indices = right_indices\n",
    "\n",
    "        if best_gain == -1:\n",
    "            return self._majority_class(y)\n",
    "\n",
    "        # Рекурсивное построение поддеревьев\n",
    "        left_tree = self._build_tree(X[best_left_indices], y[best_left_indices], depth + 1)\n",
    "        right_tree = self._build_tree(X[best_right_indices], y[best_right_indices], depth + 1)\n",
    "\n",
    "        return (best_split, left_tree, right_tree)\n",
    "\n",
    "    def _information_gain(self, y, left_indices, right_indices):\n",
    "        p = float(len(left_indices)) / len(y)\n",
    "        return self._gini(y) - p * self._gini(y[left_indices]) - (1 - p) * self._gini(y[right_indices])\n",
    "\n",
    "    def _gini(self, y):\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / len(y)\n",
    "        return 1 - np.sum(probabilities ** 2)\n",
    "\n",
    "    def _majority_class(self, y):\n",
    "        return np.bincount(y).argmax()\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = [self._predict_sample(sample, self.tree) for sample in X]\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def _predict_sample(self, sample, tree):\n",
    "        if isinstance(tree, tuple):  # Если это узел дерева\n",
    "            feature_index, threshold = tree[0]\n",
    "            if sample[feature_index] <= threshold:\n",
    "                return self._predict_sample(sample, tree[1])\n",
    "            else:\n",
    "                return self._predict_sample(sample, tree[2])\n",
    "        else:\n",
    "            return tree  # Если это лист дерева"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self, n_trees=100, max_depth=None):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for _ in range(self.n_trees):\n",
    "            # Бутстрэппинг: выбираем случайные подмножества данных\n",
    "            indices = np.random.choice(len(X), len(X), replace=True)\n",
    "            X_subsample, y_subsample = X[indices], y[indices]\n",
    "            tree = DecisionTree(max_depth=self.max_depth)\n",
    "            tree.fit(X_subsample, y_subsample)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Получение предсказаний от каждого дерева\n",
    "        predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        # Голосование\n",
    "        return np.array([np.bincount(p).argmax() for p in predictions.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение модели случайного леса\n",
    "model = RandomForest(n_trees=100, max_depth=5)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка производительности модели\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность: 1.0000\n",
      "Точность: 1.0000\n",
      "Полнота: 1.0000\n",
      "F1-мера: 1.0000\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f'Точность: {accuracy:.4f}')\n",
    "print(f'Точность: {precision:.4f}')\n",
    "print(f'Полнота: {recall:.4f}')\n",
    "print(f'F1-мера: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Объяснение:\n",
    "\n",
    "1. **Класс `DecisionTree`:**\n",
    "   - Метод `fit` обучает дерево, создавая рекурсивно его структуру, выбирая наилучшее разбиение по критерию Джини.\n",
    "   - Метод `_build_tree` строит дерево.\n",
    "   - Метод `_predict_sample` предсказывает класс для отдельного наблюдения.\n",
    "   \n",
    "2. **Класс `RandomForest`:**\n",
    "   - Метод `fit` создает несколько деревьев, обучая каждое из них на бутстрэппированной выборке.\n",
    "   - Метод `predict` собирает предсказания всех деревьев и использует голосование для определения итогового класса.\n",
    "\n",
    "3. **Обучение и оценка:**\n",
    "   - Набор данных Iris загружается, разбивается на обучающую и тестовую выборки.\n",
    "   - Модель случайного леса обучается и оценивается с использованием метрик точности, точности, полноты и F1-меры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение с стандартной реализацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sklearn = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_sklearn.fit(X_train, y_train)\n",
    "y_pred_sklearn = model_sklearn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность (sklearn): 1.0000\n",
      "Точность (sklearn): 1.0000\n",
      "Полнота (sklearn): 1.0000\n",
      "F1-мера (sklearn): 1.0000\n"
     ]
    }
   ],
   "source": [
    "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "precision_sklearn = precision_score(y_test, y_pred_sklearn, average='macro')\n",
    "recall_sklearn = recall_score(y_test, y_pred_sklearn, average='macro')\n",
    "f1_sklearn = f1_score(y_test, y_pred_sklearn, average='macro')\n",
    "\n",
    "print(f'Точность (sklearn): {accuracy_sklearn:.4f}')\n",
    "print(f'Точность (sklearn): {precision_sklearn:.4f}')\n",
    "print(f'Полнота (sklearn): {recall_sklearn:.4f}')\n",
    "print(f'F1-мера (sklearn): {f1_sklearn:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что результаты в двух случаях получились одинаковыми, связанно это с тем, что у нас немного признаков датасете, все они хорошо скореллированны. Для практики, по умолчанию быстрее и легче использовать стандартную реализацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты метрик показывают отличную производительность модели.\n",
    "\n",
    " Точность (Precision) 1.0000 означает, что модель правильно классифицирует все предсказанные положительные случаи.\n",
    " Полнота (Recall) 1.0000 означает, что модель обнаруживает все истинные положительные случаи.\n",
    " F1-мера 1.0000 представляет собой гармоническое среднее точности и полноты, что подтверждает идеальную производительность модели в сбалансированном виде.\n",
    "\n",
    "Это идеальный результат, который редко встречается в реальных задачах. Важно помнить, что это может быть связано с:\n",
    "\n",
    " Недостаточными данными: Возможно, модель обучена на очень простом наборе данных, где различия между классами очевидны.\n",
    " \n",
    " Переобучением: Модель может быть переобучена на обучающей выборке и не может обобщать на новые данные."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
